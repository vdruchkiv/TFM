?regexpr
str(df)
table(df$Status)
57*4
1- 11/57
11/57
8/19
knitr::opts_chunk$set(echo = TRUE)
data.dir<-"F:/2_EDU/UOC/MACHIN_LEARNING/PAC1/DATA"
results.dir<-"F:/2_EDU/UOC/MACHIN_LEARNING/PAC1/Statistics"
file1<-paste0(data.dir,"/","promoterDataSet.csv")
library(knitr)
df<-read.csv(file1,header = FALSE)
names(df)<-c("Status","Name","Seq")
one_hot1<-function(data,var){
seq=data[,var]
seq=gsub("a","1000",seq)
seq=gsub("c","0100",seq)
seq=gsub("g","0010",seq)
seq=gsub("t","0001",seq)
ls<-strsplit(seq, "")
ls<-lapply(ls,as.numeric)
x<- do.call("rbind",ls)
return(x)
}
one_hot<-one_hot1(df,"Seq")
as.character(df$Seq[1])
one_hot[1,]
count_dimers<-function(data,var){
dat<-apply(as.data.frame(data[,var]),1,function(x){
allcomb<-arrangements::permutations(c("a","c","g","t"),2,replace = TRUE)
allcomb<-apply(format(allcomb),1,paste,collapse="")
seq<-numeric(length(allcomb))
for(c in 1:length(allcomb)){
count=0
for(i in 1:(nchar(x)-1)){
if(substr(x,i,i+1)==allcomb[c])count=count+1
}
seq[c]<-count
}
names(seq)<-allcomb
return(seq)
})
return(t(dat))
}
count_dim<-count_dimers(df,"Seq")
count_dim[1,]
str(df)
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
prom_train
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
prom_train
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
print(table(prom_test$Status))
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
output_res(count_dim)
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
print(head(prom_test$Status))
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
print(head(prom_test))
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
output_res(count_dim)
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
print(table(prom_test_labels))
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
output_res(one_hot)
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
print(table(prom_test_labels))
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
output_res(count_dim)
knitr::opts_chunk$set(echo = TRUE)
data.dir<-"F:/2_EDU/UOC/MACHIN_LEARNING/PAC1/DATA"
results.dir<-"F:/2_EDU/UOC/MACHIN_LEARNING/PAC1/Statistics"
file1<-paste0(data.dir,"/","promoterDataSet.csv")
library(knitr)
df<-read.csv(file1,header = FALSE)
names(df)<-c("Status","Name","Seq")
one_hot1<-function(data,var){
seq=data[,var]
seq=gsub("a","1000",seq)
seq=gsub("c","0100",seq)
seq=gsub("g","0010",seq)
seq=gsub("t","0001",seq)
ls<-strsplit(seq, "")
ls<-lapply(ls,as.numeric)
x<- do.call("rbind",ls)
return(x)
}
one_hot<-one_hot1(df,"Seq")
as.character(df$Seq[1])
one_hot[1,]
count_dimers<-function(data,var){
dat<-apply(as.data.frame(data[,var]),1,function(x){
allcomb<-arrangements::permutations(c("a","c","g","t"),2,replace = TRUE)
allcomb<-apply(format(allcomb),1,paste,collapse="")
seq<-numeric(length(allcomb))
for(c in 1:length(allcomb)){
count=0
for(i in 1:(nchar(x)-1)){
if(substr(x,i,i+1)==allcomb[c])count=count+1
}
seq[c]<-count
}
names(seq)<-allcomb
return(seq)
})
return(t(dat))
}
count_dim<-count_dimers(df,"Seq")
count_dim[1,]
str(df)
if(!require(arrangements))install.packages("arrangements")
if(!require(gmodels))install.packages("gmodels")
if(!require(ROCR))install.packages("ROCR")
if(!require(plotROC))install.packages("plotROC")
if(!require(caret))install.packages("caret")
if(!require(class))install.packages("class")
#if(!require(e1071))install.packages("e1071")
library(plotROC)
rNames<-row.names(df)
set.seed(123)
rows_train<-sample(rNames,round(length(rNames)*0.67))
rows_test<-setdiff(rNames,rows_train)
output_res<-function(data){
prom_train <- data[as.numeric(rows_train),]
prom_test<-data[as.numeric(rows_test),]
prom_train_labels<-df[rows_train,"Status"]
prom_test_labels<-df[rows_test,"Status"]
print(table(prom_test_labels))
par(mfrow=c(2,2))
results<-matrix(,nrow = 4,ncol=7)
k=0
for(i in c(3,5,7,11)){
k=k+1
prom_test_pred <- knn(train = prom_train, test = prom_test,
cl = prom_train_labels, k = i,prob=TRUE)
prob <- attr(prom_test_pred , "prob")
prob[prom_test_pred=="-"] <- 1-prob[prom_test_pred=="-"]
pred_knn <- prediction(prob, prom_test_labels)
perf.auc <- performance(pred_knn, measure = "auc")
pred_knn <- performance(pred_knn, "tpr", "fpr")
plot(pred_knn, avg= "threshold", colorize=T, lwd=3, main=paste0("k=",i))
cm<-confusionMatrix(prom_test_pred,
prom_test_labels, positive = "+")
fp<-cm$table[2,1]
fn<-cm$table[1,2]
ce<-(fp+fn)/sum(cm$table)
acc<-(1-ce)
rec<-sensitivity(prom_test_pred, prom_test_labels,
positive = "+")
spec<-specificity(prom_test_pred, prom_test_labels,
negative = "-")
auc<-unlist(perf.auc@y.values)
results[k,]<-c(fp,fn,ce,acc,rec,spec,auc)
}
results<-as.data.frame(results)
results<-cbind(k=c(3,5,7,11),results)
names(results)<-c("k","FP","FN","Error","Accuracy","Sensitivity","Specificity","AUC")
print(kable(results,digits = 3))
}
output_res(one_hot)
output_res(count_dim)
10/(19+16)
4/(19+16)
shiny::runApp('D:/UOC/TFM/1_PathwayApp')
install.packages("data.table")
runApp('D:/UOC/TFM/1_PathwayApp')
install.packages("rlang")
runApp('D:/UOC/TFM/1_PathwayApp')
install.packages("backports")
runApp('D:/UOC/TFM/1_PathwayApp')
runApp('D:/UOC/TFM/1_PathwayApp')
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
search_kegg_organism()
?search_kegg_organism
kegg_organism()
search_kegg_organism
kegg_species
runApp('D:/UOC/TFM/1_PathwayApp')
runApp('D:/UOC/TFM/1_PathwayApp')
